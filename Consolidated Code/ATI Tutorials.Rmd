---
title: "ATI Tutorials"
author: "Oisín McNulty"
date: "27/04/2021"
output: html_document
---

# Workshop2 - Denosing experical covariances

```{r excercise setup, include=FALSE}
library(ati)
library(PortfolioAnalytics)
library(matlab)
library(corrplot)
library(tidyverse)
library(RColorBrewer)
library(skimr)
#library(learnr)
library(fontawesome)
knitr::opts_chunk$set(echo = FALSE,warning=FALSE, message=FALSE)

#library(shiny)
#library(learnr)
library(tidyverse)
library(tidyquant)
library(fontawesome)
library(RColorBrewer)
library(CLA)
library(ati)
```

## Introduction

In finance, empirical covariance matrices are often numerically ill-conditioned, as a result of small number of independent observations used to estimate a large number of parameters.  Working with those matrices directly, without treatment, is not recommended.

Even if the covariance matrix is non singular
![What is a singular matrice?](https://youtu.be/UqyN7-tRS00){width="50%" align="center"}
, and therefore invertible, the small determinant all but guarantees that the estimations error will be greatly magnified by the inversion process.

The **practical implication** is that these estimation errors cause misallocation of assets and substantial transaction costs due to unnecessary rebalancing.  Furthermore, *denoising* the matrix $\bf{XX^{'}}$ before inverting it should help reduce the variance of regression estimates, and improve the power of statistical tests of hypothesis.  For the same reason, covariance matrices derived from regressed factors (also known as factor-based covariance matrices) also require *denoising*, and should not be used without numerical treatment.  

### Before we begin
* Login to your RStudio on [Q-RaP](https://q-rap.qub.ac.uk:8787) with the credential provided by the lecturer
* If you don't want to use this instance use your local machine RStudio IDE but **it is your responsibility to keep it up-to-date**. For local set see this set-up workshop [here](https://q-rap.qub.ac.uk/set-up/)

* **Engage your Yoda growth mindset**

![](https://l.imgt.es/resource-preview-imgs/436d1438-0fa0-419f-b0fa-5923410ec22f%2Fyodagmmeme.crop_414x311_50%2C1.preview.jpg){width="30%"}

### Outline

In this workshop you can learn:

* First time set-up of git on Q-RaP RStudio 
* Creating fake portfolio data
* Examine correlations of the fake portfolio data
* Creating a function in R
* Setting up functions in R to *denoise* data using the Marcenko-Pastur distribution

```{bash, eval=FALSE}
git config --global user.email "<your github email>"
# This is the email you used to register with GitHub.
git config --global user.name "<your github username>"
```

## Simulating fake data
In quantitative finance we do not have a laboratory where we can securely experiment in an environment that is controlled.  Most financial research is carried out on *Real* or **Big World** data which is complex, misbehaves and are uncontrollable.  Experimentation in finance is achieved by simulating **Small World** data with know statistical properties which can be controlled.

Portfolio data from the **Big World** is usually insufficient to produce meaningful results, this insufficiency can be illustrated but create some **Small World** random data.  

### Ex 1: Fake portfolio data


```{r fake, exercise = TRUE}
stocks=20
trading_days=40
fake_port <- array(
  rnorm(trading_days*stocks,
        mean = 0.01,sd = 0.01),
  dim = c(trading_days,stocks)) %>% 
  as.tibble()
fake_port %>% skim()
```

> Describe the data?
**The data is a sample of individual and identically distributed stock returns for 20 stocks over 40 trading days. The sample is drawn from a random normal distribution with mean 0.01 and standard deviation 0.01.  This is the assumed data generating process of daily stock returns that the analyst has postulated.**

### Ex 3: Test your knowledge

```{r random, echo=FALSE}
"what do you expect the correlation matrix of these portfolio to look like if the are drawn to be independent and identically distributed ?"
"I expect there to be no pairwise correlation as the data is random"
"I expect there to be some real pairwise correlation as the data is random"
"I expect there to be some spurious pairwise correlation as the data is random, this is the answer!!"
"I expect there to boe some real pairwise correlation as the data is nonrandom"
```
<div id="random-hint">
**Hint:** use `?rnorm()` in the console to understand the output of this function 
</div>

##  Code pipes `%>%`
Firstly, I will introduce the process of piping code in R.  The point of the pipe is to help you write code in a way that is easier to read and understand. To see why the pipe is so useful, we’re going to explore a number of ways of writing the same code.  The pipe operator in R is `%>%` from the `magrittr` pacakge. For more details see [Hadley 2020 "R for Data Science) Chapter 18](https://r4ds.had.co.nz/pipes.html?q=piping#piping-alternatives)

* An algorithm for my morning routine
```
leave_home(get_dressed(get_out_of_bed(wake_up(me,time="6:30"),side="left"),trousers=TRUE,shirt=TRUE),car=FALSE,bike=TRUE,pandemic=FALSE)
```
* With piping
```
me %>%
  wake_up(time="6:30") %>%
  get_out_of_bed(side="left") %>%
  get_dressed(trousers=TRUE,shirt=TRUE) %>%
  leave_house(car=FALSE,bike=TRUE,pandemic=FALSE)
```
So the piping operator allows the code to be more readable and logic.

### Your turn

> Rearrange this code using piping

```{r pipeit,exercise=TRUE, exercise.setup="fake"}
## Recode this using piping 
summarise(group_by(mutate(fake_port,Type="Fake"),by="Type"),meanV1=mean(V1))
```

```{r pipeit-hint-1}
## Recode this using piping 
fake_port %>%
  mutate(Type="Fake") 
```

```{r pipeit-hint-2}
## Recode this using piping 
fake_port %>%
  mutate(Type="Fake") %>%
  group_by(Type)
```

```{r pipeit-solution}
## Recode this using piping 
fake_port %>%
  mutate(Type="Fake") %>%
  group_by(Type) %>%
  summarise(meanV1=mean(V1))
```


## Pairwise correlation of fake data

Given the fake portfolio was created by drawing independent and identically distributed random normal observations, by definition there should be no correlation between the fake stock returns.  

Write some code to evaluate and visualise the correlation of the fake portfolio returns which can be access in the object `fake_port`

```{r cor, exercise=TRUE,exercise.setup="fake"}
```

```{r cor-hint-1}
cor(fake_port)
```

```{r cor-hint-2}
cor(fake_port) %>%
  corrplot()
```

```{r cor-solution}
cor(fake_port) %>%
  corrplot(type="upper",
           method = "number",
           order="hclust",
           col=brewer.pal(n=8, name="RdYlBu"))
```

## Building `r fa("r-project")` functions

### Ex 1: simple function

R, at its heart, is a high level functional programming (FP) language. This means that it provides many tools for the creation and manipulation of functions.
> Write a function to add two numbers together then test the function with numbers 1 and 2
```{r add-fn, exercise=TRUE, Echo=TRUE}
add_numbers <- function(a, b) {
  
}
```

```{r add-fn-solution, Echo=TRUE}
# Write a function to add two numbers together
add_numbers <- function(a, b) {
 a + b 
}
add_numbers(1,2)
```

### Ex 2: Advanced function 

>Create a function in R for marcenko pastur distribution estimates
The Marcenko-Pastur distribution can be defined as:

$$\rho\left(\lambda  \right) = 
    \begin{cases} 
      \frac{T}{N}\frac{\sqrt {\left( {{\lambda _{+}} - \lambda} \right)\left( {\lambda  - {\lambda _{- }}} \right)}}{2\pi \lambda {\sigma ^2}}, & \text{if } \lambda \in [\lambda _{+},\lambda _{-}] \\
      0, & \text{if } \lambda \notin [\lambda _{+},\lambda _{-}]
     \end{cases}$$

where the maximum expected eigenvalue is $\lambda_{+}=\sigma^2(1+\sqrt{N/T})^2$ and the minimum expected eigenvalue is  $\lambda_{-}=\sigma^2(1-\sqrt{N/T})^2$

The following translates the above maths into `R` code. 
```{r mp, exercise = TRUE, Echo=TRUE}
mp_pdf<-function(var,t,m,pts) {
  q=t/m
  eMin<-var*(1-(1./q)^.5)^2 
  eMax<-var*(1+(1./q)^.5)^2 
  eVal<-linspace(eMin,eMax,pts)
  pd<-q/(2*pi*var*eVal)*((eMax-eVal)*(eVal-eMin))^.5
  pdf<-tibble(pd=pd,e=eVal) 
  return(pdf)  
}
```

### Ex 3: Test mp_pdf

> Test function to create the Marcenko Pastur distribution for the fake portfolio when the variance=1.  
```{r mp1-setup, Echo=TRUE}
mp_pdf<-function(var,t,m,pts) {
  q=t/m
  eMin<-var*(1-(1./q)^.5)^2 
  eMax<-var*(1+(1./q)^.5)^2 
  eVal<-linspace(eMin,eMax,pts)
  pd<-q/(2*pi*var*eVal)*((eMax-eVal)*(eVal-eMin))^.5
  pdf<-tibble(pd=pd,e=eVal) 
  return(pdf)  
}
```

```{r mp1, exercise=TRUE, exercise.setup="fake", Echo=TRUE}
mp<-mp_pdf(1,trading_days,stocks,stocks)
```

### Ex 4: plot distributoin

>Research how the package `ggplot2` works and then attempt to plot the distribution created earlier.

```{r mplot,exercise=TRUE, exercise.setup="mp1", Echo=TRUE}
mp %>% 
  ggplot(aes(x=e,y=pd)) + 
  geom_line()
```






# Workshop 3 Critical line algorithm


```{r setup,include=FALSE}
#Ex1
Top30<-ati::ftse30_returns_mthly
Top30tickers<-unique(Top30$symbol)
#Ex2
tidyquant::tq_get(Top30tickers,from="2020-01-01")->Top30prices
#Ex3
Top30r<-Top30prices %>%
  group_by(symbol) %>%
  tq_transmute(select = adjusted,
               mutate_fun = periodReturn,
               period="monthly",
               col_rename = "Rtn")
Top30r %>% 
  summarise(mu=mean(Rtn)) ->mu
Top30r %>% 
  spread(symbol,Rtn) %>%
  select(-date) %>% cov(use = "complete")->covar
CLftseTop30<-CLA(mu$mu,covar=covar,lB = 0,uB = 1/10)
```


## Introduction

In this tutorial, you will learn some portfolio analytics

* Import data from the web using `tidyqaunt` (pre-load for those unable to access yahoo finance)
* Calculate monthly returns from daily data
* Calculate the mean of covariance of a portfolio of monthly returns 
* Portfolio optimisation using 'Markowitz` Critical Line Algorithm 
### Background
Portfolio construction is perhaps the most recurrent financial problem.  On a daily basis, investment managers must build portfolios that incorporate their views and forecasts on risks and returns.  This is the primordial question that 24-yr-old Harry Markowitz attempted to answer more than six decades ago.  His monumental insight was to recognise that various levels of risk are associated with different optimal portfolios in terms of risk-adjusted returns, hence the notion of the **Efficient Frontier** [Markowitz (1952)](https://shibbolethsp.jstor.org/start?entityID=https%3A%2F%2Fqub.ac.uk%2Fshibboleth&dest=https://www.jstor.org/stable/2975974&site=jstor)
**One practical implication is that it is rarely optimal to allocate all assets to the investments with the highest expected returns.  Instead, we should take into account the correlations across alternative investments in order to build a diversified portfolio.  Portfolio diversification is still the key to success in investment management**.  
> If you buy shares in an umbrella company and a maker of sunglasses, you will be fine in all weathers `r tufte::quote_footer("Messy by Tim Harford")`

### CLA alogirithm
Before earning his PhD in 1954, Markowitz left academia to work at the RAND corporation, where he developed the Critical Line Algorithm.  CLA is a quadratic optimisation procedure specifically designed for inequality-constrained portfolio optimisation problems.  The beauty of this algorithm is that it guarantees that the exact solution is found after a known number of iterations.  Surprisingly, most financial practitioners still seem unaware of CLA, as they often rely on generic-purpose quadratic programming methods that do not guarantee the correction solution or a stopping time.

### The optimisation problem

In Modern Portfolio Theory, this operation consists in computing the Efficient Frontier, defined as the set of portfolios that yield the highest achievable mean excess return (in excess of the risk-free rate) for any given level of risk(measured in terms of standard deviation).
This portfolio optimization problem receives two equivalent formulations:
1. Minimizing the portfolio’s standard deviation(or variance) subject to a targeted excess return or
2. Maximize the portfolio’s excess return subject to a targeted standard deviation (or variance). 

[This problem is a challenge for all practitioners in the Global Asset Management industry which in 2019 had total assets under management (AuM) $89 trillion](https://www.bcg.com/en-gb/publications/2020/global-asset-management-protect-adapt-innovate_)

### The mathematical challenge
Most practitioners are routinely faced with the problem of optimizing a portfolio subject to inequality conditions (a lower and an upper bound for each portfolio weight) and an equality condition (that the weights add up to one). There is no analytic solution to this problem, and an optimization algorithm must be used. Markowitz developed a method for computing such a solution, which he named the “critical line algorithm”or CLA.

## Data preprocessing
In this exercise we will using the Top 30 FTSE holdings monthly returns to find the minimum variance portfolio that would maximise the return over the last year.

### load data

>Firstly, I loaded the list of top 30 holdings into R [Top 30 holdings](https://uk.finance.yahoo.com/quote/%5EFTSE/components?p=%5EFTSE) and save the output to an object named `Top30`. Then create a character vector of the tickers.
```{r CLA1, exercise = TRUE}
Top30 %>% glimpse()
```


```{r CLA1-solution}
Top30tickers<-Top30$Symbol
```


